## Summary
While AVX-512 is most visibly an extension of AVX and AVX2 to a 512 bit width, 27% of AVX-512 intrinsics are 128 or 256 bits wide. Skylake architecture CPUs with AVX-512 enabled support 3959 of the 5139 AVX-512 intrinsics defined by Intel. Ice Lake expands the set to 4130 intrinsics and Alder Lake to 5095 intrinsics. Alder Lake appears to support 100% the of intrinsics which are not obsolete.

| group   | what                          | instructions | intrinsics | Alder Lake | Sunny Cove | Skylake      | Knights | VL intrinsics |
| ------- | ----------------------------- | ------------ | ---------- | -----------|----------- | ------------ | ------- | ------------- |
| F       | foundation                    |  389         | 1435       | P-core     | Ice Lake   | X, Xeon      | all     |               |
| CD      | conflict detection            |    8         |   42       | P-core     | Ice Lake   | X, Xeon      | all     |   28          |
| VL      | 128 and 256 bit widths        |  223         | 1208       | P-core     | Ice Lake   | X, Xeon      |         | 1208          |
| DQ      | doubleword and quadword       |   87         |  399       | P-core     | Ice Lake   | X, Xeon      |         |  176          |
| BW      | byte and word                 |  150         |  764       | P-core     | Ice Lake   | X, Xeon      |         |  446          |
| BITALG  | population count expansion    |    5         |   24       | P-core     | Ice Lake   |              |         |   16          |
| IFMA52  | big integer FMA               |    3         |   18       | P-core     | Ice Lake   | Cannon Lake  |         |   12          |
| VBMI    | vector byte manipulation      |    8         |   30       | P-core     | Ice Lake   | Cannon Lake  |         |   20          |
| VBMI2   | vector byte manipulation      |   21         |  150       | P-core     | Ice Lake   |              |         |  100          |
| VNNI    | vector neural network         |    5         |   36       | P-core     | Ice Lake   | Cascade Lake |         |   24          |
| VPOPCNT | population count              |    3         |   18       | P-core     | Ice Lake   |              | Mill    |   12          |
| BF16    | half precision                |    5         |   27       | P-core     |            |              |         |   18          |
| FP16    | half precision                |   96         |  938       | P-core     |            |              |         |  600          |
| VP2INTERSECT | vector pair to mask pair |    2         |    6       | P-core     |            |              |         |    4          |
| 4FMAPS  | single precision 4x1 FMA      |    4         |   12       |            |            |              | Mill    |               |
| 4NNIW   | vector neural network         |    2         |    6       |            |            |              | Mill    |               |
| ER      | exponential and reciprocal    |   12         |   60       |            |            |              | all     |               |
| PF      | prefetch                      |    9         |   20       |            |            |              | all     |               |
| total   |                               | 1031         | 5193       |            |            |              |         | 2060          |

Prior to i3, i5, and i7 Ice Lakes, AVX-512 was available mainly on Skylake and Cascade Lake i9s and Xeons. Tiger Lake Core CPUs and Cooper Lake Xeons provide AVX-512 but, as of the end of 2021, AVX-512 availability remains low to due to limited Ice and Tiger Lake shipments. Intel indicated AVX-512 would be disabled on Alder Lake CPUs, implying 12<sup>th</sup> generation AVX-512 availability would be limited to Sapphire Rapids Xeons, but it's been found some motherboards support AVX-512 on Alder Lake P-cores [when Alder Lake i9 E-cores are disabled](https://www.anandtech.com/show/17047/the-intel-12th-gen-core-i912900k-review-hybrid-performance-brings-hybrid-complexity/2). AVX-512 was first available on the now discontinued Xeon Phis (Knights Landing and Knights Mill), though their support is limited to compared to Skylake and later implementations. The i3-8121U, the only Cannon Lake processor, shipped in low volume. AMD does not currently implement AVX-512.

The table above derives from the [Intel Intrinsics Guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/), [Intel ARK](https://ark.intel.com/content/www/us/en/ark/search/featurefilter.html?productType=873&1_Filter-InstructionSetExtensions=3533), and [Intel 64 and IA-32 Architectures Software Developer's Manuals](https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html). It will therefore be inaccurate if Intel's information is inaccurate or if transcription errors were made. In particular, sections 15.2-4 of the architecture manual, volume 1, require software check for F before using other groups. However, the Intrinsics Guide does not indicate corresponding dependencies for many groups.

## Instruction Group Dependencies and CPUID Flags
The 18 AVX-512 instruction groups have individual CPUID flags and, in principle, an instruction could require an arbitrary number of groups be present. In practice, this is rarely a concern as the only dependencies which exist are on groups F and VL. Since VL is not present independent of F on any of Intel's processors, its dependency is always satisfied. Similarly, all processors with instruction groups containing VL dependent intrinsics implement VL. There are also 324 128 or 256 bit intrinsics for which the Intrinsics Guide does not indicate a VL requirement. These are primarily ss and sd floating point intrinsics which modify only the low 32 or 64 bits of a register.

It appears reasonable to assume all Skylake and later processors with AVX-512 support will implement at least the F, CD, VL, DQ, and BW groups. While Intel could choose otherwise, doing would complicate hardware implementation, compiler support, and software compatibility. It is also plausible the BITALG, IFMA52, VBMI, VBMI2, VNNI, and VPOPCNT groups will be consistently implemented from Ice Lake forward for the same reasons. However, Intel does not seem to have made any official statement regarding future compatibility. Similarly, AMD does not appear to have indicated what groups a Zen 4 implementation might support. However, an AMD implementation seems likely to be broadly compatible with Intel's.

Since AVX-512 is restricted to 512 bit widths on the now discontinued Xeon Phis, these processors do not implement the vector length extensions of the VL group. They therefore lack dependencies between groups and share only the F and CD groups with Skylake and later implementions. The 4NNIW group appears to be replaced by VNNI and the PF group by architectural changes.

## Performance Considerations
The Skylake, Cascade Lake, and Sunny Cove microarchitectures provide SIMD computation on ports 0, 1, and 5. It appears AVX-512 operation is obtained by combining ports 0 and 1 and, when two AVX-512 instructions per clock are supported, possibly by combining ports 5 and 6. In addition to the downclocking and voltage regulator step up latencies associated with operations wider than 128 bits, use of AVX-512 may sometimes be slower than AVX-256 for the same workload. This occurs because the instruction rate decrease from 3x256 per clock to 2x512 may not be offset by wider loads and stores ([Fog 2015](https://www.agner.org/optimize/blog/read.php?i=628#415), [Stackoverflow](https://stackoverflow.com/questions/15655835/flops-per-cycle-for-sandy-bridge-and-haswell-sse2-avx-avx2)), zmm register availability, or use of more efficient instructions. AVX-512 can also be disadvantageous on processors restricted to 1x512, which includes bronze, silver, some 5000 series gold, and D Skylake Xeons as well as Knights processors. 

In general, compute kernel throughput is sensitive to instruction level parallelism available within the kernel's inner loop and a processor's FMA, ALU, shift, floating point divide, shuffle, and load and store capabilities. For Ice Lake Core CPUs, Intel indicates one AVX-512 FMA and shuffle and two AVX-512 ALUs (*e.g.* [Cutress 2019](https://www.anandtech.com/show/14514/examining-intels-ice-lake-microarchitecture-and-sunny-cove/12)). Some kernels may therefore execute more quickly at 256 bit width due to accessing two FMA and shuffle units rather than one. Additionally, in some cases 128 bit kernels can be faster than 256 or 512 bit ones due to access to higher turbo and base clocks or computational details of the kernel such as dependencies between AVX lanes.

It's therefore useful to profile SIMD implementations at 128, 256, and 512 bit widths across processors and potentially also the amount of computation to be performed. In performance sensitive applications, this can result in width dispatching as well as more traditional CPU dispatching. Broadly, this suggests use of the 1400 128 and 256 bit AVX-512 intrinsics is likely to be at least as important than the 3700 512 bit ones.
